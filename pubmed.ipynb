{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchNormal(driver, keyword:str):\n",
    "    selector_search = \"input#id_term\"\n",
    "    search = driver.find_element(By.CSS_SELECTOR, selector_search)\n",
    "    search.click()\n",
    "    search.send_keys(keyword)\n",
    "    search.send_keys(Keys.ENTER)\n",
    "\n",
    "def searchAdvance(driver, keyword:str):\n",
    "    to_advance = driver.find_element_by_css_selector('a.search-input-link')\n",
    "    to_advance.click()\n",
    "\n",
    "    # title과 abstract으로 설정\n",
    "    title_abstrac = driver.find_element_by_css_selector('select#field-selector')\n",
    "    title_abstrac.click()\n",
    "    opt = driver.find_element_by_css_selector('#field-selector > option:nth-child(39)')\n",
    "    opt.click()\n",
    "\n",
    "    # Input Keywords\n",
    "    search = driver.find_element_by_css_selector('input#id_term')\n",
    "    search.click()\n",
    "    search.send_keys(keyword)\n",
    "    search.send_keys(Keys.ENTER)\n",
    "\n",
    "    searchBtn = driver.find_element_by_css_selector('#search-form > div > div > div.query-box-section-wrapper > div.button-wrapper > button')\n",
    "    searchBtn.click()\n",
    "\n",
    "def checkBlank(list: list):\n",
    "    return \"\" if len(list) ==0 else list.text\n",
    "\n",
    "def post_processing_text(text):\n",
    "    return text.strip().replace('\\n', '').replace('\\t', '') if text is not None else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pubmed_crawling(keyword, advance = False):\n",
    "    driver = wd.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(\"https://pubmed.ncbi.nlm.nih.gov\") \n",
    "\n",
    "    # Keywords input\n",
    "    if advance: # advance search\n",
    "        searchAdvance(driver, keyword)\n",
    "    else:\n",
    "        searchNormal(driver, keyword)\n",
    "\n",
    "    # Page to the end\n",
    "    selector_show_more_btn = \"#search-results > section > div.search-results-paginator.next-results-paginator.has-nav > button\"\n",
    "    show_more_btn = driver.find_elements(By.CSS_SELECTOR, selector_show_more_btn)\n",
    "    while True:\n",
    "        try:\n",
    "            show_more_btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector_show_more_btn)))\n",
    "            show_more_btn.click()\n",
    "        except :\n",
    "            print(\"The last page\")\n",
    "            break\n",
    "\n",
    "    # Collect URLs\n",
    "    url = \"https://pubmed.ncbi.nlm.nih.gov\"\n",
    "    html_source = driver.page_source\n",
    "    soup = BeautifulSoup(html_source, 'lxml')\n",
    "    selector_title = \"div.search-results-chunk > article.full-docsum > div.docsum-wrap > div.docsum-content > a.docsum-title\"\n",
    "    urls = [url + a['href'] for a in soup.select(selector_title)]\n",
    "\n",
    "    # Crawling HTML sources\n",
    "    html_sources = []\n",
    "\n",
    "    for url in tqdm(urls, desc=\"Processing HTML sources\"):\n",
    "        driver = wd.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "        driver.get(url)\n",
    "        html_source = driver.page_source\n",
    "        html_sources.append(html_source)\n",
    "    print(\"Crawling source finishhed!\")\n",
    "\n",
    "    # Crawling abstracts\n",
    "    crawling_result_list = []\n",
    "    selector_title = \"h1.heading-title\"\n",
    "    selector_author = \"div.inline-authors > .authors > .authors-list >  span.authors-list-item > a.full-name\"\n",
    "    selector_journal = \"div.article-citation > div > div > button\"\n",
    "    selector_year = \"div.article-citation > div.article-source > span.cit\"\n",
    "    selector_doi = \"#full-view-identifiers > li:nth-child(3) > span > a.id-link\"\n",
    "    selector_abstract = \"div.abstract-content\"\n",
    "    for source in tqdm(html_sources, desc=\"Processing abstracts\"):\n",
    "        soup = BeautifulSoup(source, 'lxml')\n",
    "        title_list = soup.select(selector_title);\n",
    "        author_list = soup.select(selector_author)\n",
    "        journal_list = soup.select(selector_journal)\n",
    "        year_list = soup.select(selector_year)\n",
    "        doi_list = soup.select(selector_doi) \n",
    "        abstract_list = soup.select(selector_abstract)\n",
    "        \n",
    "        title_edit = post_processing_text(title_list[0].text) \n",
    "        author_edit = post_processing_text(author_list[0].text) if len(author_list) != 0 else \"\"\n",
    "        journal_edit = post_processing_text(journal_list[0].text) if len(journal_list) != 0 else \"\"\n",
    "        year_edit = post_processing_text(year_list[0].text) if len(year_list) != 0 else \"\"\n",
    "        doi_edit = post_processing_text(doi_list[0].text) if len(doi_list) != 0 else \"\"\n",
    "        abstract_edit = post_processing_text(abstract_list[0].text) if len(abstract_list)!=0 else \"\"\n",
    "        abstract_dict = {\"title\": title_edit, \"author\": author_edit + \"et al.\", \"journal\": journal_edit,\n",
    "                        \"year\": year_edit, \"doi\": doi_edit, \"abstract\": abstract_edit}\n",
    "        crawling_result_list.append(abstract_dict)  \n",
    "    #     for title, author, journal, year, doi, abstract in zip(title_list, author_list, journal_list, year_list, doi_list, abstract_list):\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(crawling_result_list)\n",
    "    if not Path(\"Pubmed/\").exists():\n",
    "        Path.mkdir(\"Pubmed/\")\n",
    "    else:\n",
    "        pass\n",
    "    df.to_csv(f\"Pubmed/Pubmed_{keyword}.csv\")\n",
    "    print(\"Job finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HTML sources: 100%|██████████| 16/16 [01:16<00:00,  4.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling source finishhed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing abstracts: 100%|██████████| 16/16 [00:01<00:00, 12.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "keyword = \"AKI AND sepsis AND bundle\"\n",
    "result = pubmed_crawling(keyword = keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
